{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624e79fc",
   "metadata": {},
   "source": [
    "# Fabric Capacity Utilization Monitoring\n",
    "\n",
    "**Production monitoring solution** that collects capacity metrics and workload data from Microsoft Fabric REST APIs and sends to Azure Log Analytics.\n",
    "\n",
    "## What This Monitors\n",
    "- âœ… **Capacity Inventory**: Basic capacity details, workspace counts, item distribution\n",
    "- âœ… **Workload Data**: Item-based workload tracking across all workspaces\n",
    "- âœ… **Workspace Events**: Workspace lifecycle and item details\n",
    "\n",
    "## Data Streams\n",
    "- `Custom-FabricCapacityMetrics_CL` - Capacity utilization metrics\n",
    "- `Custom-FabricCapacityWorkloads_CL` - Workload and item inventory\n",
    "- `Custom-FabricWorkspaceEvents_CL` - Workspace events and changes\n",
    "- `Custom-FabricItemDetails_CL` - Enhanced item metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Framework Integration for Capacity Monitoring ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the framework to path  \n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import framework components\n",
    "from fabricla_connector.config import get_config\n",
    "from fabricla_connector.api import FabricAPIClient\n",
    "from fabricla_connector.collectors import CapacityCollector\n",
    "from fabricla_connector.ingestion import FabricIngestion\n",
    "from fabricla_connector.utils import create_time_window\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load configuration\n",
    "load_dotenv()\n",
    "config = get_config()\n",
    "\n",
    "print(\"âœ… FabricLA Connector framework loaded for capacity monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Framework-Based Configuration for Capacity Monitoring ===\n",
    "\n",
    "# Configuration parameters\n",
    "capacity_ids = [\n",
    "    # Add your capacity IDs here, e.g.:\n",
    "    \"12345678-1234-1234-1234-123456789abc\",\n",
    "]\n",
    "\n",
    "# Time window configuration\n",
    "lookback_hours = 24  # Monitor last 24 hours of capacity usage\n",
    "\n",
    "# Stream name for capacity data (must match DCR configuration)\n",
    "stream_capacity = \"Custom-FabricCapacityUtilization_CL\"\n",
    "\n",
    "# Validation\n",
    "workspace_id = config.get(\"FABRIC_WORKSPACE_ID\")\n",
    "if not workspace_id:\n",
    "    print(\"âš ï¸  Warning: FABRIC_WORKSPACE_ID not set - some features may be limited\")\n",
    "\n",
    "required_vars = [\"FABRIC_TENANT_ID\", \"FABRIC_APP_ID\", \"AZURE_MONITOR_DCE_ENDPOINT\", \"AZURE_MONITOR_DCR_IMMUTABLE_ID\"]\n",
    "missing = [var for var in required_vars if not config.get(var)]\n",
    "\n",
    "if missing:\n",
    "    print(f\"âŒ Missing required environment variables: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"âœ… Framework configuration validated\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Capacity Monitoring Configuration:\")\n",
    "print(f\"  Capacities to monitor: {len(capacity_ids)}\")\n",
    "print(f\"  Lookback period: {lookback_hours} hours\")\n",
    "print(f\"  Stream name: {stream_capacity}\")\n",
    "print(f\"  DCE Endpoint: {config.get('AZURE_MONITOR_DCE_ENDPOINT', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Capacity Utilization Data Collection and Ingestion ===\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "print(\"ðŸš€ Starting capacity utilization data collection...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize framework components\n",
    "try:\n",
    "    fabric_client = FabricAPIClient()\n",
    "    capacity_collector = CapacityCollector(fabric_client)\n",
    "    \n",
    "    # Initialize ingestion client\n",
    "    ingestion_client = FabricIngestion(\n",
    "        endpoint_host=f\"https://{dcr_endpoint_host}\",\n",
    "        dcr_id=dcr_immutable_id,\n",
    "        stream_name=stream_capacity\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Framework components initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Framework initialization failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize summary for tracking results\n",
    "summary = {\n",
    "    \"collection_timestamp\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"lookback_hours\": lookback_hours,\n",
    "    \"workspace_id\": workspace_id,\n",
    "    \"configuration\": {\n",
    "        \"dcr_endpoint_host\": dcr_endpoint_host,\n",
    "        \"dcr_immutable_id\": dcr_immutable_id,\n",
    "        \"capacity_items\": len(capacity_ids)\n",
    "    }\n",
    "}\n",
    "\n",
    "# === COLLECT CAPACITY UTILIZATION DATA ===\n",
    "capacity_rows = []\n",
    "\n",
    "if capacity_ids:\n",
    "    print(f\"\\nâš¡ Collecting capacity utilization for {len(capacity_ids)} capacities...\")\n",
    "    \n",
    "    for capacity_id in capacity_ids:\n",
    "        print(f\"   Collecting from capacity: {capacity_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect capacity metrics\n",
    "            metrics = capacity_collector.collect_capacity_metrics(\n",
    "                capacity_id=capacity_id,\n",
    "                lookback_hours=lookback_hours\n",
    "            )\n",
    "            capacity_rows.extend(metrics)\n",
    "            print(f\"     Found {len(metrics)} metric records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     âŒ Error collecting from capacity {capacity_id}: {e}\")\n",
    "            \n",
    "    print(f\"âœ… Capacity collection completed: {len(capacity_rows)} records\")\n",
    "else:\n",
    "    print(\"â­ï¸  No capacity IDs configured - skipping capacity collection\")\n",
    "\n",
    "# === INGEST DATA TO AZURE MONITOR ===\n",
    "if capacity_rows:\n",
    "    print(f\"\\nðŸ“¤ Ingesting {len(capacity_rows)} capacity utilization records...\")\n",
    "    \n",
    "    try:\n",
    "        capacity_result = ingestion_client.ingest_enhanced(\n",
    "            records=capacity_rows,\n",
    "            troubleshoot=True\n",
    "        )\n",
    "        \n",
    "        summary[\"capacity_metrics\"] = {\n",
    "            \"collected\": len(capacity_rows),\n",
    "            \"ingested\": capacity_result.get(\"successful_records\", 0),\n",
    "            \"failed\": capacity_result.get(\"failed_records\", 0),\n",
    "            \"success_rate\": capacity_result.get(\"success_rate\", 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"     âœ… Capacity metrics: {capacity_result.get('successful_records', 0)}/{len(capacity_rows)} ingested\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ Capacity ingestion failed: {e}\")\n",
    "        summary[\"capacity_metrics\"] = {\"collected\": len(capacity_rows), \"ingested\": 0, \"error\": str(e)}\n",
    "else:\n",
    "    print(\"â­ï¸  No capacity utilization records to ingest\")\n",
    "    summary[\"capacity_metrics\"] = {\"collected\": 0, \"ingested\": 0}\n",
    "\n",
    "# === SUMMARY REPORT ===\n",
    "total_collected = len(capacity_rows)\n",
    "total_ingested = summary.get(\"capacity_metrics\", {}).get(\"ingested\", 0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… CAPACITY UTILIZATION MONITORING COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“Š Total records collected: {total_collected}\")\n",
    "print(f\"ðŸ“¤ Total records ingested: {total_ingested}\")\n",
    "print(f\"âš¡ Capacity metrics: {total_ingested}\")\n",
    "\n",
    "if total_ingested > 0:\n",
    "    print(f\"\\nðŸŽ¯ Data should appear in Log Analytics within 5-15 minutes\")\n",
    "    print(f\"   Table: {stream_capacity} â†’ FabricCapacityUtilization_CL\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No records ingested - check your capacity IDs and configuration\")\n",
    "\n",
    "print(f\"\\nðŸ“„ Detailed summary available in 'summary' variable\")\n",
    "print(json.dumps(summary, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Capacity Monitoring Troubleshooting ===\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "print(\"ðŸ” CAPACITY UTILIZATION MONITORING TROUBLESHOOTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Display summary\n",
    "if 'summary' in locals():\n",
    "    print(\"\\n1. COLLECTION & INGESTION SUMMARY:\")\n",
    "    print(json.dumps(summary, indent=2, default=str))\n",
    "    \n",
    "    total_collected = summary.get(\"capacity_metrics\", {}).get(\"collected\", 0)\n",
    "    total_ingested = summary.get(\"capacity_metrics\", {}).get(\"ingested\", 0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Summary Statistics:\")\n",
    "    print(f\"   Total collected: {total_collected}\")\n",
    "    print(f\"   Total ingested: {total_ingested}\")\n",
    "    print(f\"   Success rate: {(total_ingested/total_collected*100) if total_collected > 0 else 0:.1f}%\")\n",
    "    \n",
    "    if total_ingested == 0:\n",
    "        print(\"\\nâŒ No data was ingested\")\n",
    "        print(\"ðŸ” Possible causes:\")\n",
    "        print(\"   - Invalid capacity IDs\")\n",
    "        print(\"   - Insufficient permissions to access capacity metrics\")\n",
    "        print(\"   - No capacity utilization during lookback window\")\n",
    "        print(\"   - Authentication issues\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… {total_ingested} capacity utilization records ingested successfully\")\n",
    "else:\n",
    "    print(\"âŒ No summary data available - run data collection cell first\")\n",
    "\n",
    "# 2. Configuration verification\n",
    "print(\"\\n2. CONFIGURATION VERIFICATION:\")\n",
    "print(f\"   Workspace ID: {workspace_id or 'Not set (some capacity APIs may not require this)'}\")\n",
    "print(f\"   Capacity count: {len(capacity_ids)}\")\n",
    "print(f\"   Lookback period: {lookback_hours} hours\")\n",
    "print(f\"   DCR Endpoint: https://{dcr_endpoint_host}\")\n",
    "print(f\"   Stream: {stream_capacity}\")\n",
    "\n",
    "# 3. KQL queries for verification\n",
    "print(\"\\n3. KQL VERIFICATION QUERIES:\")\n",
    "print(\"   Run these queries in your Log Analytics workspace:\")\n",
    "print(\"   ```kql\")\n",
    "print(\"   // Check capacity utilization table\")\n",
    "print(\"   FabricCapacityUtilization_CL | count\")\n",
    "print()\n",
    "print(\"   // Recent capacity metrics\")\n",
    "print(\"   FabricCapacityUtilization_CL\")\n",
    "print(\"   | where TimeGenerated > ago(24h)\")\n",
    "print(\"   | project TimeGenerated, CapacityId = capacity_id_s, CPUUtilization = cpu_utilization_d, MemoryUtilization = memory_utilization_d\")\n",
    "print(\"   | order by TimeGenerated desc\")\n",
    "print(\"   | take 20\")\n",
    "print()\n",
    "print(\"   // Capacity utilization trends\")\n",
    "print(\"   FabricCapacityUtilization_CL\")\n",
    "print(\"   | where TimeGenerated > ago(7d)\")\n",
    "print(\"   | summarize\")\n",
    "print(\"       AvgCPU = avg(cpu_utilization_d),\")\n",
    "print(\"       MaxCPU = max(cpu_utilization_d),\")\n",
    "print(\"       AvgMemory = avg(memory_utilization_d),\")\n",
    "print(\"       MaxMemory = max(memory_utilization_d)\")\n",
    "print(\"   by capacity_id_s, bin(TimeGenerated, 1h)\")\n",
    "print(\"   | order by TimeGenerated desc\")\n",
    "print()\n",
    "print(\"   // Peak usage identification\")\n",
    "print(\"   FabricCapacityUtilization_CL\")\n",
    "print(\"   | where TimeGenerated > ago(24h)\")\n",
    "print(\"   | where cpu_utilization_d > 80 or memory_utilization_d > 80\")\n",
    "print(\"   | project TimeGenerated, capacity_id_s, cpu_utilization_d, memory_utilization_d\")\n",
    "print(\"   | order by cpu_utilization_d desc\")\n",
    "print(\"   ```\")\n",
    "\n",
    "# 4. Sample data inspection\n",
    "if 'capacity_rows' in locals() and capacity_rows:\n",
    "    print(\"\\n4. SAMPLE CAPACITY DATA:\")\n",
    "    sample = capacity_rows[0]\n",
    "    print(\"   First capacity utilization record structure:\")\n",
    "    for key, value in list(sample.items())[:8]:  # Show first 8 fields\n",
    "        print(f\"     {key}: {value}\")\n",
    "    if len(sample) > 8:\n",
    "        print(f\"   ... and {len(sample)-8} more fields\")\n",
    "\n",
    "# 5. Capacity monitoring insights\n",
    "print(\"\\n5. CAPACITY MONITORING INSIGHTS:\")\n",
    "print(\"   ðŸ“Š Key metrics to monitor in Log Analytics:\")\n",
    "print(\"   - CPU and memory utilization trends\")\n",
    "print(\"   - Peak usage times and resource bottlenecks\")\n",
    "print(\"   - Capacity scaling requirements\")\n",
    "print(\"   - Performance correlation with workload patterns\")\n",
    "print(\"   - Cost optimization opportunities\")\n",
    "\n",
    "print(\"\\n6. CAPACITY OPTIMIZATION RECOMMENDATIONS:\")\n",
    "print(\"   âš¡ Based on utilization data:\")\n",
    "print(\"   - Identify optimal capacity sizing\")\n",
    "print(\"   - Schedule workloads during low-usage periods\")\n",
    "print(\"   - Monitor for consistent over/under-utilization\")\n",
    "print(\"   - Plan capacity scaling based on growth trends\")\n",
    "print(\"   - Optimize workload distribution across capacities\")\n",
    "\n",
    "# 7. Alerting suggestions\n",
    "print(\"\\n7. SUGGESTED ALERTS:\")\n",
    "print(\"   ðŸš¨ Create Log Analytics alerts for:\")\n",
    "print(\"   - CPU utilization > 90% for sustained periods\")\n",
    "print(\"   - Memory utilization > 95%\")\n",
    "print(\"   - Unusual capacity usage spikes\")\n",
    "print(\"   - Capacity unavailability or errors\")\n",
    "\n",
    "# 8. Next steps\n",
    "print(\"\\n8. RECOMMENDED NEXT STEPS:\")\n",
    "if 'summary' in locals() and total_ingested > 0:\n",
    "    print(\"   âœ… Data collection successful\")\n",
    "    print(\"   ðŸ“Š Build capacity utilization dashboards\")\n",
    "    print(\"   ðŸš¨ Set up alerting for high utilization\")\n",
    "    print(\"   ðŸ“ˆ Analyze usage patterns for optimization\")\n",
    "    print(\"   ðŸ’° Monitor for cost optimization opportunities\")\n",
    "else:\n",
    "    print(\"   âŒ Investigate data collection issues:\")\n",
    "    print(\"   1. Verify capacity IDs are correct and accessible\")\n",
    "    print(\"   2. Check admin permissions for capacity metrics\")\n",
    "    print(\"   3. Verify capacity is active and has utilization data\")\n",
    "    print(\"   4. Test with a larger lookback window\")\n",
    "\n",
    "print(f\"\\nâ° Troubleshooting completed at: {dt.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47430153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Advanced Capacity Analytics and Best Practices ===\n",
    "\n",
    "print(\"\udcca ADVANCED CAPACITY ANALYTICS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sample advanced analytics queries for capacity optimization\n",
    "advanced_queries = {\n",
    "    \"Peak Hours Analysis\": \"\"\"\n",
    "    FabricCapacityUtilization_CL\n",
    "    | where TimeGenerated > ago(30d)\n",
    "    | extend Hour = datetime_part('hour', TimeGenerated)\n",
    "    | summarize AvgCPU = avg(cpu_utilization_d), AvgMemory = avg(memory_utilization_d) by Hour\n",
    "    | order by AvgCPU desc\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Capacity Right-sizing\": \"\"\"\n",
    "    FabricCapacityUtilization_CL\n",
    "    | where TimeGenerated > ago(7d)\n",
    "    | summarize \n",
    "        P95_CPU = percentile(cpu_utilization_d, 95),\n",
    "        P95_Memory = percentile(memory_utilization_d, 95),\n",
    "        Avg_CPU = avg(cpu_utilization_d),\n",
    "        Avg_Memory = avg(memory_utilization_d)\n",
    "    by capacity_id_s\n",
    "    | extend \n",
    "        CPURecommendation = case(\n",
    "            P95_CPU > 90, \"Scale Up - High utilization\",\n",
    "            P95_CPU < 30, \"Scale Down - Low utilization\", \n",
    "            \"Optimal sizing\"\n",
    "        )\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Cost Optimization\": \"\"\"\n",
    "    FabricCapacityUtilization_CL\n",
    "    | where TimeGenerated > ago(24h)\n",
    "    | where cpu_utilization_d < 20 and memory_utilization_d < 20\n",
    "    | summarize LowUtilizationHours = count() by capacity_id_s\n",
    "    | where LowUtilizationHours > 12\n",
    "    | project capacity_id_s, LowUtilizationHours, PotentialSavings = \"Consider downsizing\"\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Workload Correlation\": \"\"\"\n",
    "    // Correlate capacity usage with pipeline/dataflow runs\n",
    "    FabricCapacityUtilization_CL\n",
    "    | where TimeGenerated > ago(24h)\n",
    "    | join kind=inner (\n",
    "        FabricPipelineRun_CL\n",
    "        | where TimeGenerated > ago(24h)\n",
    "        | summarize PipelineRuns = count() by bin(TimeGenerated, 1h)\n",
    "    ) on $left.TimeGenerated\n",
    "    | project TimeGenerated, cpu_utilization_d, memory_utilization_d, PipelineRuns\n",
    "    | order by TimeGenerated desc\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ” Advanced Analytics Query Examples:\")\n",
    "for title, query in advanced_queries.items():\n",
    "    print(f\"\\nðŸ“ˆ {title}:\")\n",
    "    print(\"```kql\")\n",
    "    print(query.strip())\n",
    "    print(\"```\")\n",
    "\n",
    "print(\"\\nðŸ’¡ CAPACITY MONITORING BEST PRACTICES:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "best_practices = [\n",
    "    \"ðŸ“Š Monitor both CPU and memory utilization patterns\",\n",
    "    \"â° Track peak usage times to optimize workload scheduling\", \n",
    "    \"ðŸ“ˆ Use percentile analysis (P95, P99) for capacity planning\",\n",
    "    \"ðŸ”„ Correlate capacity usage with specific workloads\",\n",
    "    \"ðŸ’° Identify cost optimization opportunities through low utilization periods\",\n",
    "    \"ðŸš¨ Set up proactive alerts before reaching capacity limits\",\n",
    "    \"ðŸ“‹ Regularly review capacity allocation vs actual usage\",\n",
    "    \"ðŸŽ¯ Plan capacity scaling based on business growth projections\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"  {practice}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CAPACITY PLANNING GUIDELINES:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"  ðŸ“Š Utilization Targets:\")\n",
    "print(\"    - CPU: Target 60-80% average, 90% peak maximum\")\n",
    "print(\"    - Memory: Target 70-85% average, 95% peak maximum\")\n",
    "print(\"  \")\n",
    "print(\"  âš¡ Scaling Triggers:\")\n",
    "print(\"    - Scale up: >90% utilization for >2 hours\")\n",
    "print(\"    - Scale down: <30% utilization for >24 hours\")\n",
    "print(\"  \")\n",
    "print(\"  ðŸ’° Cost Optimization:\")\n",
    "print(\"    - Review capacities with <20% utilization\")\n",
    "print(\"    - Consider auto-scaling for variable workloads\")\n",
    "print(\"    - Schedule non-critical workloads during off-peak hours\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Regular Capacity Health Checks:\")\n",
    "print(\"  ðŸ” Weekly Reviews:\")\n",
    "print(\"    - Capacity utilization trends\")\n",
    "print(\"    - Peak usage identification\")\n",
    "print(\"    - Performance bottleneck analysis\")\n",
    "print(\"  \")\n",
    "print(\"  ðŸ“ˆ Monthly Planning:\")\n",
    "print(\"    - Capacity growth requirements\")\n",
    "print(\"    - Cost optimization opportunities\")\n",
    "print(\"    - Workload distribution optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d32b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "print(\"ðŸ“Š Starting data collection...\")\n",
    "\n",
    "# Get base data\n",
    "print(\"   ðŸ¢ Fetching capacities...\")\n",
    "all_capacities = get_capacities(fabric_token)\n",
    "\n",
    "print(\"   ðŸ“ Fetching workspaces...\")\n",
    "all_workspaces = get_workspaces(fabric_token)\n",
    "\n",
    "# Filter capacities if specific IDs provided\n",
    "if capacity_ids:\n",
    "    target_ids = [cid.lower() for cid in capacity_ids]\n",
    "    capacities_to_monitor = [cap for cap in all_capacities \n",
    "                           if cap.get('id', '').lower() in target_ids]\n",
    "    print(f\"   ðŸŽ¯ Filtered to {len(capacities_to_monitor)} specific capacities\")\n",
    "else:\n",
    "    capacities_to_monitor = all_capacities\n",
    "    print(f\"   ðŸŒ Monitoring all {len(capacities_to_monitor)} accessible capacities\")\n",
    "\n",
    "# Collect monitoring data\n",
    "print(\"   ðŸ“ˆ Collecting capacity metrics...\")\n",
    "capacity_metrics = collect_capacity_metrics(capacities_to_monitor, all_workspaces)\n",
    "\n",
    "print(\"   ðŸ’¼ Collecting workload data...\")\n",
    "workload_data = collect_workspace_workloads(all_workspaces, fabric_token)\n",
    "\n",
    "print(\"   ðŸ“‹ Collecting workspace events...\")\n",
    "workspace_events = collect_workspace_events(all_workspaces, fabric_token)\n",
    "\n",
    "print(\"   ðŸ“¦ Collecting item details...\")\n",
    "item_details = collect_item_details(all_workspaces, fabric_token)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nðŸ“Š Collection Summary:\")\n",
    "print(f\"   Capacity metrics: {len(capacity_metrics)} records\")\n",
    "print(f\"   Workload data: {len(workload_data)} records\")\n",
    "print(f\"   Workspace events: {len(workspace_events)} records\")\n",
    "print(f\"   Item details: {len(item_details)} records\")\n",
    "\n",
    "print(\"âœ… Data collection completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send Data to Log Analytics\n",
    "print(\"ðŸ“¤ Sending data to Azure Log Analytics...\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Send capacity metrics\n",
    "if capacity_metrics:\n",
    "    print(f\"   ðŸ“ˆ Sending {len(capacity_metrics)} capacity metrics...\")\n",
    "    result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                 stream_capacity_metrics, capacity_metrics, monitor_token)\n",
    "    results[\"capacity_metrics\"] = result\n",
    "    print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "\n",
    "# Send workload data\n",
    "if workload_data:\n",
    "    print(f\"   ðŸ’¼ Sending {len(workload_data)} workload records...\")\n",
    "    result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                 stream_capacity_workloads, workload_data, monitor_token)\n",
    "    results[\"workload_data\"] = result\n",
    "    print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "\n",
    "# Send workspace events (if DCR stream exists)\n",
    "if workspace_events:\n",
    "    try:\n",
    "        print(f\"   ðŸ“‹ Sending {len(workspace_events)} workspace events...\")\n",
    "        result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                     stream_workspace_events, workspace_events, monitor_token)\n",
    "        results[\"workspace_events\"] = result\n",
    "        print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "    except RuntimeError as e:\n",
    "        if \"not configured\" in str(e):\n",
    "            print(f\"      âš ï¸  Stream {stream_workspace_events} not configured in DCR - skipping\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Send item details (if DCR stream exists)\n",
    "if item_details:\n",
    "    try:\n",
    "        print(f\"   ðŸ“¦ Sending {len(item_details)} item details...\")\n",
    "        result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                     stream_item_details, item_details, monitor_token)\n",
    "        results[\"item_details\"] = result\n",
    "        print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "    except RuntimeError as e:\n",
    "        if \"not configured\" in str(e):\n",
    "            print(f\"      âš ï¸  Stream {stream_item_details} not configured in DCR - skipping\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"\\nâœ… Data ingestion completed successfully!\")\n",
    "print(f\"\\nðŸ“‹ Final Results:\")\n",
    "for stream_type, result in results.items():\n",
    "    print(f\"   {stream_type}: {result['sent']} records sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501308f6",
   "metadata": {},
   "source": [
    "## âœ… Monitoring Complete\n",
    "\n",
    "### Infrastructure Setup\n",
    "**Important**: Before running this notebook, deploy the required Azure infrastructure using:\n",
    "- **Terraform**: `logAnalytics/terraform/` folder\n",
    "- **Bicep**: `logAnalytics/bicep/` folder\n",
    "\n",
    "Both create:\n",
    "- Log Analytics Workspace with custom tables\n",
    "- Data Collection Endpoint (DCE)\n",
    "- Data Collection Rules (DCR) with all required streams\n",
    "\n",
    "### Data Collected\n",
    "- **Capacity Metrics**: Basic capacity inventory with workspace counts\n",
    "- **Workload Data**: Item-based workload tracking across workspaces  \n",
    "- **Workspace Events**: Workspace inventory and item distribution\n",
    "- **Item Details**: Enhanced metadata for all Fabric items\n",
    "\n",
    "### Next Steps\n",
    "1. **Deploy Infrastructure**: Use Terraform or Bicep templates first\n",
    "2. **Configure Environment**: Set DCR_ENDPOINT_HOST and DCR_IMMUTABLE_ID from deployment outputs\n",
    "3. **Schedule this notebook** to run regularly (daily/weekly)\n",
    "4. **Build dashboards** in Azure Monitor/Power BI using the collected data\n",
    "5. **Set up alerts** based on capacity utilization trends\n",
    "\n",
    "### KQL Query Examples\n",
    "```kql\n",
    "// Capacity utilization overview\n",
    "FabricCapacityMetrics_CL\n",
    "| summarize arg_max(TimeGenerated, *) by CapacityId\n",
    "| project CapacityName, CapacityType, WorkspaceCount, Region\n",
    "\n",
    "// Item distribution by type\n",
    "FabricCapacityWorkloads_CL\n",
    "| summarize count() by WorkloadType, CapacityId\n",
    "| order by count_ desc\n",
    "\n",
    "// Workspace activity\n",
    "FabricWorkspaceEvents_CL\n",
    "| where TimeGenerated > ago(7d)\n",
    "| summarize TotalItems = sum(ItemCount) by WorkspaceName\n",
    "```\n",
    "\n",
    "### Infrastructure Files\n",
    "- `logAnalytics/terraform/` - Terraform templates for Azure resources\n",
    "- `logAnalytics/bicep/` - Bicep templates for Azure resources\n",
    "- `logAnalytics/common/` - Shared JSON templates for table and DCR definitions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
