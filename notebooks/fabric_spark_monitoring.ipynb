{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aef5c8f",
   "metadata": {},
   "source": [
    "# Fabric Spark Monitoring with Log Analytics Integration\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive Spark monitoring for Microsoft Fabric using **(Livy Sessions)** and **(Resource Usage)** APIs, with data ingestion into Azure Monitor Log Analytics.\n",
    "\n",
    "### ðŸŽ¯ What This Notebook Collects\n",
    "\n",
    "#### Livy Sessions Monitoring\n",
    "1. **Workspace Livy Sessions** - All interactive Spark sessions in workspace\n",
    "2. **Notebook Sessions** - Spark sessions launched from notebooks\n",
    "3. **Spark Job Sessions** - Sessions from Spark job definitions\n",
    "4. **Lakehouse Sessions** - Sessions accessing Lakehouse data\n",
    "5. **Session Logs** - Driver and executor logs per session\n",
    "6. **History Metrics** - Performance metrics from Spark History Server\n",
    "\n",
    "#### Resource Usage Monitoring\n",
    "7. **Spark Resource Usage** - CPU, memory, disk, network metrics\n",
    "8. **Active Session Resources** - Real-time resource tracking\n",
    "9. **Bottleneck Detection** - Identify CPU/memory/disk/network issues\n",
    "10. **Capacity Analysis** - Executor efficiency and resource utilization\n",
    "\n",
    "### ðŸ“Š Data Ingestion Streams\n",
    "All data is sent to Azure Monitor via Data Collection Rules (DCR):\n",
    "- `FabricSparkLivySession_CL` - Session metadata (17 columns)\n",
    "- `FabricSparkLogs_CL` - Driver/executor logs (13 columns)\n",
    "- `FabricSparkHistoryMetrics_CL` - Performance metrics (19 columns)\n",
    "- `FabricSparkResourceUsage_CL` - Resource utilization (24 columns)\n",
    "\n",
    "### Prerequisites\n",
    "- âœ… Azure authentication (DefaultAzureCredential or service principal)\n",
    "- âœ… Infrastructure deployed (DCE, DCR, Log Analytics workspace)\n",
    "- âœ… `.env` file configured with Azure Monitor settings\n",
    "- âœ… Fabric workspace with active Spark sessions\n",
    "\n",
    "### Value Proposition\n",
    "- ðŸŽ¯ **Complete Spark Observability** - Sessions, logs, metrics, resources\n",
    "- ðŸ“ˆ **Performance Analysis** - Identify slow jobs, resource bottlenecks\n",
    "- ðŸš¨ **Proactive Alerting** - Detect failures and resource exhaustion\n",
    "- ðŸ’° **Cost Optimization** - Track executor usage and capacity waste\n",
    "- ðŸ“Š **Historical Trends** - Long-term performance and capacity planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d1930",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab21255",
   "metadata": {},
   "source": [
    "### Load Environment Configuration\n",
    "\n",
    "Load environment variables from `.env` file and verify Azure Monitor configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb87ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“¦ FABRICLA-CONNECTOR SPARK MONITORING\n",
      "================================================================================\n",
      "âœ… Modules imported successfully\n",
      "âœ… Phase 1 APIs: Livy Sessions, Logs, Metrics\n",
      "âœ… Phase 4 APIs: Resource Usage, Bottleneck Detection\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import FabricLA-Connector components\n",
    "from fabricla_connector import workflows\n",
    "from fabricla_connector.config import validate_config\n",
    "from fabricla_connector.collectors import (\n",
    "    # Livy Sessions collectors\n",
    "    collect_livy_sessions_workspace,\n",
    "    collect_livy_sessions_notebook,\n",
    "    collect_livy_sessions_sparkjob,\n",
    "    collect_livy_sessions_lakehouse,\n",
    "    collect_spark_logs,\n",
    "    collect_spark_metrics,\n",
    "    # Resource Usage collectors\n",
    "    collect_spark_resource_usage,\n",
    "    collect_resource_usage_for_active_sessions,\n",
    "    # General Spark collectors\n",
    "    collect_spark_applications_workspace,\n",
    "    collect_spark_applications_item,\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“¦ FABRICLA-CONNECTOR SPARK MONITORING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ… Modules imported successfully\")\n",
    "print(\"âœ… Phase 1 APIs: Livy Sessions, Logs, Metrics\")\n",
    "print(\"âœ… Phase 4 APIs: Resource Usage, Bottleneck Detection\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa4f4ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded environment variables from: c:\\Dvlp\\fabric-la-connector\\notebooks\\.env\n"
     ]
    }
   ],
   "source": [
    "# Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Determine the path to .env file in the notebooks directory\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in dir() else os.getcwd()\n",
    "env_path = os.path.join(notebook_dir, \".env\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path, override=True)\n",
    "    print(f\"âœ… Loaded environment variables from: {env_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ .env file not found at: {env_path}\")\n",
    "    print(f\"   Current directory: {os.getcwd()}\")\n",
    "    print(f\"   Please ensure .env file exists in the notebooks directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20182128",
   "metadata": {},
   "source": [
    "### Validate Configuration\n",
    "\n",
    "Verify Azure Monitor and Fabric workspace settings are properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c408fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Validating configuration...\n",
      "\n",
      "Configuration Status: âœ… Valid\n",
      "\n",
      "ðŸ”§ Azure Monitor Configuration:\n",
      "   DCE Endpoint: https://dce-fabric-monitoring-lede.canadacentral-1...\n",
      "   DCR ID: dcr-6987822159f748c38d622d990a60351c\n",
      "   Log Analytics: law-fabric-monitoring\n",
      "\n",
      "ðŸŽ¯ Target workspace: YourWorkspaceName (8457f746-f2d9-4d27-8221-5714601e40c6)\n",
      "\n",
      "ðŸ“Š Collection Settings:\n",
      "   Lookback: 43200 hours\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration and validation\n",
    "print(\"âš™ï¸ Validating configuration...\\n\")\n",
    "\n",
    "# Validate all configuration sections\n",
    "config_status = validate_config(\"all\")\n",
    "print(f\"Configuration Status: {'âœ… Valid' if config_status else 'âŒ Invalid'}\\n\")\n",
    "\n",
    "# Check Azure Monitor configuration\n",
    "dce_endpoint = os.getenv(\"AZURE_MONITOR_DCE_ENDPOINT\")\n",
    "dcr_id = os.getenv(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\")\n",
    "workspace_name = os.getenv(\"LOG_ANALYTICS_WORKSPACE_NAME\")\n",
    "\n",
    "print(\"ðŸ”§ Azure Monitor Configuration:\")\n",
    "print(\n",
    "    f\"   DCE Endpoint: {dce_endpoint[:50]}...\"\n",
    "    if dce_endpoint\n",
    "    else \"   DCE Endpoint: âŒ Not configured\"\n",
    ")\n",
    "print(f\"   DCR ID: {dcr_id}\" if dcr_id else \"   DCR ID: âŒ Not configured\")\n",
    "print(\n",
    "    f\"   Log Analytics: {workspace_name}\"\n",
    "    if workspace_name\n",
    "    else \"   Log Analytics: âŒ Not configured\"\n",
    ")\n",
    "\n",
    "# Set workspace ID (update with your workspace ID)\n",
    "WORKSPACE_ID = os.getenv(\"FABRIC_WORKSPACE_ID\", \"your-workspace-id-here\")\n",
    "WORKSPACE_NAME = os.getenv(\"FABRIC_WORKSPACE_NAME\", \"YourWorkspace\")\n",
    "\n",
    "if WORKSPACE_ID == \"your-workspace-id-here\":\n",
    "    print(\"\\nâš ï¸ Please update FABRIC_WORKSPACE_ID in .env file\")\n",
    "else:\n",
    "    print(f\"\\nðŸŽ¯ Target workspace: {WORKSPACE_NAME} ({WORKSPACE_ID})\")\n",
    "\n",
    "# Collection configuration\n",
    "LOOKBACK_HOURS = int(os.getenv(\"FABRIC_LOOKBACK_HOURS\", \"24\"))\n",
    "\n",
    "print(f\"\\nðŸ“Š Collection Settings:\")\n",
    "print(f\"   Lookback: {LOOKBACK_HOURS} hours\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab97c34",
   "metadata": {},
   "source": [
    "### Validate Spark Availability\n",
    "\n",
    "Quick check to verify Spark is enabled in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736afdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Spark availability...\n",
      "Workspace: 8457f746-f2d9-4d27-8221-5714601e40c6\n",
      "Endpoint: /v1/workspaces/8457f746-f2d9-4d27-8221-5714601e40c6/spark/livySessions\n",
      "\n",
      "[Auth] Fabric authentication not available: No module named 'notebookutils'\n",
      "[Auth] Using service principal authentication\n",
      "[Auth] SUCCESS: Using credentials from Environment Variables\n",
      "SUCCESS: Token acquired for https://api.fabric.microsoft.com/.default: eyJ0eXAiOi...bQVLaWGHbA\n",
      "SUCCESS: Token acquired for https://api.fabric.microsoft.com/.default: eyJ0eXAiOi...bQVLaWGHbA\n",
      "Found 14 Livy sessions\n",
      "Collected 0 sessions\n",
      "SUCCESS - Spark API accessible\n",
      "Sessions found: 0\n",
      "\n",
      "Note: No active sessions in last hour (this is normal)\n",
      "      Collections will work once Spark sessions exist\n",
      "\n",
      "============================================================\n",
      "READY\n",
      "============================================================\n",
      "Found 14 Livy sessions\n",
      "Collected 0 sessions\n",
      "SUCCESS - Spark API accessible\n",
      "Sessions found: 0\n",
      "\n",
      "Note: No active sessions in last hour (this is normal)\n",
      "      Collections will work once Spark sessions exist\n",
      "\n",
      "============================================================\n",
      "READY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Validating Spark availability\n",
    "print(\"Validating Spark availability...\")\n",
    "print(f\"Workspace: {WORKSPACE_ID}\")\n",
    "print(f\"Endpoint: /v1/workspaces/{WORKSPACE_ID}/spark/livySessions\\n\")\n",
    "\n",
    "try:\n",
    "    test_generator = collect_livy_sessions_workspace(\n",
    "        workspace_id=WORKSPACE_ID, \n",
    "        lookback_hours=1\n",
    "    )\n",
    "    \n",
    "    test_sessions = list(test_generator)\n",
    "    \n",
    "    print(f\"SUCCESS - Spark API accessible\")\n",
    "    print(f\"Sessions found: {len(test_sessions)}\\n\")\n",
    "    \n",
    "    if len(test_sessions) == 0:\n",
    "        print(\"Note: No active sessions in last hour (this is normal)\")\n",
    "        print(\"      Collections will work once Spark sessions exist\\n\")\n",
    "    \n",
    "    SPARK_AVAILABLE = True\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"ERROR: {error_msg}\\n\")\n",
    "    \n",
    "    if \"401\" in error_msg or \"403\" in error_msg:\n",
    "        SPARK_AVAILABLE = False\n",
    "        print(\"Authentication issue - check service principal permissions\")\n",
    "    elif \"404\" in error_msg:\n",
    "        print(\"Workspace not found or Spark not enabled\")\n",
    "        print(\"Will attempt collection anyway...\")\n",
    "        SPARK_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"Unexpected error - will attempt collection anyway\")\n",
    "        SPARK_AVAILABLE = True\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"READY\" if SPARK_AVAILABLE else \"BLOCKED - Fix auth first\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e733ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Phase 1: Livy Sessions Collection\n",
    "\n",
    "### 1.1 Workspace Livy Sessions\n",
    "Collect all interactive Spark sessions in the workspace (notebooks, spark jobs, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45625ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Livy Sessions Collection\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1.1: WORKSPACE LIVY SESSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not SPARK_AVAILABLE:\n",
    "    print(\"Skipped - Spark not available\")\n",
    "    workspace_sessions = []\n",
    "else:\n",
    "    try:\n",
    "        session_generator = collect_livy_sessions_workspace(\n",
    "            workspace_id=WORKSPACE_ID, lookback_hours=LOOKBACK_HOURS\n",
    "        )\n",
    "\n",
    "        workspace_sessions = list(session_generator)\n",
    "        print(f\"\\nCollection complete: {len(workspace_sessions)} sessions\")\n",
    "\n",
    "        # Ingest to Azure Monitor\n",
    "        if workspace_sessions:\n",
    "            from fabricla_connector.ingestion import FabricIngestion\n",
    "\n",
    "            dce_endpoint = os.getenv(\"AZURE_MONITOR_DCE_ENDPOINT\")\n",
    "            dcr_id = os.getenv(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\")\n",
    "            stream_name = os.getenv(\n",
    "                \"AZURE_MONITOR_STREAM_LIVY_SESSION\", \"Custom-FabricSparkLivySession_CL\"\n",
    "            )\n",
    "\n",
    "            ingestion = FabricIngestion(\n",
    "                endpoint_host=dce_endpoint, dcr_id=dcr_id, stream_name=stream_name\n",
    "            )\n",
    "\n",
    "            ingestion_result = ingestion.ingest_enhanced(\n",
    "                records=workspace_sessions, troubleshoot=True\n",
    "            )\n",
    "\n",
    "            print(f\"Ingested: {ingestion_result.get('successful_records', 0)}\")\n",
    "            if ingestion_result.get(\"failed_records\", 0) > 0:\n",
    "                print(f\"Failed: {ingestion_result.get('failed_records', 0)}\")\n",
    "\n",
    "            # Display session states\n",
    "            if workspace_sessions:\n",
    "                states = {}\n",
    "                for session in workspace_sessions:\n",
    "                    state = session.get(\"State\", \"unknown\")\n",
    "                    states[state] = states.get(state, 0) + 1\n",
    "                print(\"\\nSession states:\")\n",
    "                for state, count in states.items():\n",
    "                    print(f\"  {state}: {count}\")\n",
    "        else:\n",
    "            print(\"No sessions to ingest\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "\n",
    "        if \"404\" in error_msg:\n",
    "            print(\"\\nSpark not enabled or no sessions exist\")\n",
    "            print(\"Run a Spark notebook first, then retry\")\n",
    "        elif \"401\" in error_msg or \"403\" in error_msg:\n",
    "            print(\"\\nAuthentication issue - check credentials\")\n",
    "        \n",
    "        workspace_sessions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ccea2",
   "metadata": {},
   "source": [
    "### 1.2 Notebook Livy Sessions\n",
    "Collect Spark sessions specifically launched from Fabric notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Livy Sessions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ““ PHASE 1.2: NOTEBOOK LIVY SESSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not SPARK_AVAILABLE:\n",
    "    print(\"â­ï¸ Skipped - Spark is not available in this workspace\")\n",
    "else:\n",
    "    try:\n",
    "        # Get a notebook ID from your workspace (update with actual notebook ID)\n",
    "        NOTEBOOK_ID = os.getenv(\"FABRIC_NOTEBOOK_ID\", \"skip\")\n",
    "\n",
    "        if NOTEBOOK_ID != \"skip\":\n",
    "            # Collect session data from the generator\n",
    "            session_generator = collect_livy_sessions_notebook(\n",
    "                workspace_id=WORKSPACE_ID,\n",
    "                notebook_id=NOTEBOOK_ID,\n",
    "                lookback_hours=LOOKBACK_HOURS,\n",
    "            )\n",
    "\n",
    "            # Convert generator to list\n",
    "            notebook_sessions = list(session_generator)\n",
    "\n",
    "            print(f\"âœ… Notebook sessions collected!\")\n",
    "            print(f\"   Sessions found: {len(notebook_sessions)}\")\n",
    "\n",
    "            # Ingest to Azure Monitor\n",
    "            if notebook_sessions:\n",
    "                from fabricla_connector.ingestion import FabricIngestion\n",
    "\n",
    "                dce_endpoint = os.getenv(\"AZURE_MONITOR_DCE_ENDPOINT\")\n",
    "                dcr_id = os.getenv(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\")\n",
    "                stream_name = os.getenv(\n",
    "                    \"AZURE_MONITOR_STREAM_LIVY_SESSION\",\n",
    "                    \"Custom-FabricSparkLivySession_CL\",\n",
    "                )\n",
    "\n",
    "                ingestion = FabricIngestion(\n",
    "                    endpoint_host=dce_endpoint, dcr_id=dcr_id, stream_name=stream_name\n",
    "                )\n",
    "\n",
    "                ingestion_result = ingestion.ingest_enhanced(\n",
    "                    records=notebook_sessions, troubleshoot=True\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"   Records ingested: {ingestion_result.get('successful_records', 0)}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   â„¹ï¸ No sessions found for this notebook\")\n",
    "        else:\n",
    "            print(\n",
    "                \"â­ï¸ Skipped - Set FABRIC_NOTEBOOK_ID in .env to collect notebook sessions\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"âŒ Error: {error_msg}\")\n",
    "        if \"404\" in error_msg or \"EntityNotFound\" in error_msg:\n",
    "            print(\n",
    "                f\"   ðŸ’¡ Tip: Verify the notebook ID exists and has run Spark sessions\"\n",
    "            )\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20733851",
   "metadata": {},
   "source": [
    "### 1.3 Spark Job Livy Sessions\n",
    "Collect sessions from Spark job definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Job Livy Sessions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âš¡ PHASE 1.3: SPARK JOB LIVY SESSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not SPARK_AVAILABLE:\n",
    "    print(\"â­ï¸ Skipped - Spark is not available in this workspace\")\n",
    "else:\n",
    "    try:\n",
    "        SPARKJOB_ID = os.getenv(\"FABRIC_SPARKJOB_ID\", \"skip\")\n",
    "\n",
    "        if SPARKJOB_ID != \"skip\":\n",
    "            # Collect session data from the generator\n",
    "            session_generator = collect_livy_sessions_sparkjob(\n",
    "                workspace_id=WORKSPACE_ID,\n",
    "                sparkjob_id=SPARKJOB_ID,\n",
    "                lookback_hours=LOOKBACK_HOURS,\n",
    "            )\n",
    "\n",
    "            # Convert generator to list\n",
    "            sparkjob_sessions = list(session_generator)\n",
    "\n",
    "            print(f\"âœ… Spark job sessions collected!\")\n",
    "            print(f\"   Sessions found: {len(sparkjob_sessions)}\")\n",
    "\n",
    "            # Ingest to Azure Monitor\n",
    "            if sparkjob_sessions:\n",
    "                from fabricla_connector.ingestion import FabricIngestion\n",
    "\n",
    "                dce_endpoint = os.getenv(\"AZURE_MONITOR_DCE_ENDPOINT\")\n",
    "                dcr_id = os.getenv(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\")\n",
    "                stream_name = os.getenv(\n",
    "                    \"AZURE_MONITOR_STREAM_LIVY_SESSION\",\n",
    "                    \"Custom-FabricSparkLivySession_CL\",\n",
    "                )\n",
    "\n",
    "                ingestion = FabricIngestion(\n",
    "                    endpoint_host=dce_endpoint, dcr_id=dcr_id, stream_name=stream_name\n",
    "                )\n",
    "\n",
    "                ingestion_result = ingestion.ingest_enhanced(\n",
    "                    records=sparkjob_sessions, troubleshoot=True\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"   Records ingested: {ingestion_result.get('successful_records', 0)}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   â„¹ï¸ No sessions found for this Spark job\")\n",
    "        else:\n",
    "            print(\n",
    "                \"â­ï¸ Skipped - Set FABRIC_SPARKJOB_ID in .env to collect spark job sessions\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"âŒ Error: {error_msg}\")\n",
    "        if \"404\" in error_msg or \"EntityNotFound\" in error_msg:\n",
    "            print(f\"   ðŸ’¡ Tip: Verify the Spark job ID exists and has been executed\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdad95b",
   "metadata": {},
   "source": [
    "### 1.4 Lakehouse Livy Sessions\n",
    "Collect sessions accessing Lakehouse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca28936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakehouse Livy Sessions \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ  PHASE 1.4: LAKEHOUSE LIVY SESSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not SPARK_AVAILABLE:\n",
    "    print(\"â­ï¸ Skipped - Spark is not available in this workspace\")\n",
    "else:\n",
    "    try:\n",
    "        LAKEHOUSE_ID = os.getenv(\"FABRIC_LAKEHOUSE_ID\", \"skip\")\n",
    "        LAKEHOUSE_NAME = os.getenv(\"FABRIC_LAKEHOUSE_NAME\", \"DefaultLakehouse\")\n",
    "\n",
    "        if LAKEHOUSE_ID != \"skip\":\n",
    "            # Collect session data from the generator\n",
    "            session_generator = collect_livy_sessions_lakehouse(\n",
    "                workspace_id=WORKSPACE_ID,\n",
    "                lakehouse_id=LAKEHOUSE_ID,\n",
    "                lakehouse_name=LAKEHOUSE_NAME,\n",
    "                workspace_name=WORKSPACE_NAME,\n",
    "                lookback_hours=LOOKBACK_HOURS,\n",
    "            )\n",
    "\n",
    "            # Convert generator to list\n",
    "            lakehouse_sessions = list(session_generator)\n",
    "\n",
    "            print(f\"âœ… Lakehouse sessions collected!\")\n",
    "            print(f\"   Sessions found: {len(lakehouse_sessions)}\")\n",
    "\n",
    "            # Ingest to Azure Monitor\n",
    "            if lakehouse_sessions:\n",
    "                from fabricla_connector.ingestion import FabricIngestion\n",
    "\n",
    "                dce_endpoint = os.getenv(\"AZURE_MONITOR_DCE_ENDPOINT\")\n",
    "                dcr_id = os.getenv(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\")\n",
    "                stream_name = os.getenv(\n",
    "                    \"AZURE_MONITOR_STREAM_LIVY_SESSION\",\n",
    "                    \"Custom-FabricSparkLivySession_CL\",\n",
    "                )\n",
    "\n",
    "                ingestion = FabricIngestion(\n",
    "                    endpoint_host=dce_endpoint, dcr_id=dcr_id, stream_name=stream_name\n",
    "                )\n",
    "\n",
    "                ingestion_result = ingestion.ingest_enhanced(\n",
    "                    records=lakehouse_sessions, troubleshoot=True\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"   Records ingested: {ingestion_result.get('successful_records', 0)}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   â„¹ï¸ No sessions found for this Lakehouse\")\n",
    "        else:\n",
    "            print(\n",
    "                \"â­ï¸ Skipped - Set FABRIC_LAKEHOUSE_ID in .env to collect lakehouse sessions\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"âŒ Error: {error_msg}\")\n",
    "        if \"404\" in error_msg or \"EntityNotFound\" in error_msg:\n",
    "            print(\n",
    "                f\"   ðŸ’¡ Tip: Verify the Lakehouse ID exists and has been accessed by Spark\"\n",
    "            )\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8552a6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Phase 2: Resource Usage Monitoring\n",
    "\n",
    "### 2.1 Spark Resource Usage Collection\n",
    "Collect comprehensive resource metrics (CPU, memory, disk, network) for all Spark sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31824289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’» PHASE 4.1: SPARK RESOURCE USAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not SPARK_AVAILABLE:\n",
    "    print(\"â­ï¸ Skipped - Spark is not available in this workspace\")\n",
    "else:\n",
    "    try:\n",
    "        # Note: collect_resource_usage_for_active_sessions collects resources for all active sessions\n",
    "        # It internally calls collect_livy_sessions_workspace first\n",
    "        resource_generator = collect_resource_usage_for_active_sessions(\n",
    "            workspace_id=WORKSPACE_ID, lookback_hours=LOOKBACK_HOURS\n",
    "        )\n",
    "\n",
    "        # Convert generator to list\n",
    "        resource_records = list(resource_generator)\n",
    "\n",
    "        print(f\"âœ… Resource usage collected!\")\n",
    "        print(f\"   Resource records: {len(resource_records)}\")\n",
    "\n",
    "        # Ingest to Azure Monitor\n",
    "        if resource_records:\n",
    "            from fabricla_connector.ingestion import FabricIngestion\n",
    "\n",
    "            dce_endpoint = os.getenv(\"AZURE_MONITOR_DCE_ENDPOINT\")\n",
    "            dcr_id = os.getenv(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\")\n",
    "            stream_name = os.getenv(\n",
    "                \"AZURE_MONITOR_STREAM_RESOURCE_USAGE\",\n",
    "                \"Custom-FabricSparkResourceUsage_CL\",\n",
    "            )\n",
    "\n",
    "            ingestion = FabricIngestion(\n",
    "                endpoint_host=dce_endpoint, dcr_id=dcr_id, stream_name=stream_name\n",
    "            )\n",
    "\n",
    "            ingestion_result = ingestion.ingest_enhanced(\n",
    "                records=resource_records, troubleshoot=True\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"   Records ingested: {ingestion_result.get('successful_records', 0)}\"\n",
    "            )\n",
    "            print(f\"   Failed: {ingestion_result.get('failed_records', 0)}\")\n",
    "\n",
    "            # Display resource summary\n",
    "            print(f\"\\nðŸ“Š Resource Summary:\")\n",
    "            total_cpu = sum(float(r.get(\"TotalCPUCores\", 0)) for r in resource_records)\n",
    "            total_mem = sum(float(r.get(\"TotalMemoryGB\", 0)) for r in resource_records)\n",
    "            total_disk = sum(float(r.get(\"TotalDiskGB\", 0)) for r in resource_records)\n",
    "\n",
    "            print(f\"   Total CPU cores: {total_cpu:.1f}\")\n",
    "            print(f\"   Total Memory: {total_mem:.2f} GB\")\n",
    "            print(f\"   Total Disk: {total_disk:.2f} GB\")\n",
    "\n",
    "            # Show bottleneck analysis\n",
    "            bottlenecks = {}\n",
    "            for r in resource_records:\n",
    "                bottleneck = r.get(\"BottleneckType\", \"none\")\n",
    "                if bottleneck and bottleneck != \"none\":\n",
    "                    bottlenecks[bottleneck] = bottlenecks.get(bottleneck, 0) + 1\n",
    "\n",
    "            if bottlenecks:\n",
    "                print(f\"\\nâš ï¸ Bottlenecks detected:\")\n",
    "                for btype, count in bottlenecks.items():\n",
    "                    print(f\"   {btype}: {count} sessions\")\n",
    "        else:\n",
    "            print(f\"   â„¹ï¸ No resource data available\")\n",
    "            print(\n",
    "                f\"   ðŸ’¡ Tip: Resource usage requires sessions with running Spark applications\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"âŒ Error: {error_msg}\")\n",
    "        if \"404\" in error_msg or \"EntityNotFound\" in error_msg:\n",
    "            print(f\"   ðŸ’¡ Tip: No active Spark sessions found for resource monitoring\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4035c",
   "metadata": {},
   "source": [
    "### 2.2 Active Sessions Resource Tracking\n",
    "Real-time resource monitoring for currently active Spark sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"â±ï¸ PHASE 4.2: ACTIVE SESSION RESOURCES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not SPARK_AVAILABLE:\n",
    "    print(\"â­ï¸ Skipped - Spark is not available in this workspace\")\n",
    "else:\n",
    "    try:\n",
    "        # This is the same as 4.1 but we'll filter to show only currently active\n",
    "        resource_generator = collect_resource_usage_for_active_sessions(\n",
    "            workspace_id=WORKSPACE_ID, lookback_hours=LOOKBACK_HOURS\n",
    "        )\n",
    "\n",
    "        # Convert generator to list\n",
    "        active_resources = list(resource_generator)\n",
    "\n",
    "        print(f\"âœ… Active session resources collected!\")\n",
    "        print(f\"   Resource records: {len(active_resources)}\")\n",
    "\n",
    "        if active_resources:\n",
    "            # Note: Data already ingested in Phase 4.1 if same function was used\n",
    "            # This cell focuses on displaying active session details\n",
    "\n",
    "            print(f\"\\nðŸ”¥ Currently Active Sessions:\")\n",
    "            for i, resource in enumerate(active_resources[:10], 1):  # Show first 10\n",
    "                session_id = resource.get(\"SessionId\", \"unknown\")\n",
    "                state = resource.get(\"State\", \"unknown\")\n",
    "                cpu = float(resource.get(\"TotalCPUCores\", 0))\n",
    "                mem = float(resource.get(\"TotalMemoryGB\", 0))\n",
    "                efficiency = float(resource.get(\"ExecutorEfficiency\", 0))\n",
    "                bottleneck = resource.get(\"BottleneckType\", \"none\")\n",
    "\n",
    "                print(f\"\\n   {i}. Session {session_id[:12]}... [{state}]\")\n",
    "                print(f\"      CPU: {cpu:.1f} cores, Memory: {mem:.2f} GB\")\n",
    "                print(f\"      Executor Efficiency: {efficiency:.1f}%\")\n",
    "                if bottleneck and bottleneck != \"none\":\n",
    "                    severity = float(resource.get(\"BottleneckSeverity\", 0))\n",
    "                    print(\n",
    "                        f\"      âš ï¸ Bottleneck: {bottleneck} (severity: {severity:.2f})\"\n",
    "                    )\n",
    "\n",
    "            if len(active_resources) > 10:\n",
    "                print(f\"\\n   ... and {len(active_resources) - 10} more active sessions\")\n",
    "        else:\n",
    "            print(f\"   â„¹ï¸ No active sessions with resource data at this time\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"âŒ Error: {error_msg}\")\n",
    "        if \"404\" in error_msg or \"EntityNotFound\" in error_msg:\n",
    "            print(f\"   ðŸ’¡ Tip: No active Spark sessions found for resource monitoring\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324748ba",
   "metadata": {},
   "source": [
    "## ðŸ“Š Collection Complete\n",
    "\n",
    "Your Spark monitoring data has been collected and ingested to Azure Monitor Log Analytics.\n",
    "\n",
    "### Next Steps\n",
    "1. **Wait 2-5 minutes** for data to appear in Log Analytics\n",
    "2. **Query your data** using KQL in Azure Portal â†’ Log Analytics\n",
    "3. **Create dashboards** for visualizing Spark performance\n",
    "4. **Set up alerts** for failures and resource bottlenecks\n",
    "5. **Schedule this notebook** to run periodically (every 15-30 minutes)\n",
    "\n",
    "### Sample KQL Queries\n",
    "\n",
    "#### Query 1: Recent Livy Sessions\n",
    "```kql\n",
    "FabricSparkLivySession_CL\n",
    "| where TimeGenerated > ago(24h)\n",
    "| project TimeGenerated, WorkspaceName, SessionName, State, ExecutorCount, ExecutorCores\n",
    "| order by TimeGenerated desc\n",
    "```\n",
    "\n",
    "#### Query 2: Resource Usage by Session\n",
    "```kql\n",
    "FabricSparkResourceUsage_CL\n",
    "| where TimeGenerated > ago(24h)\n",
    "| summarize \n",
    "    AvgCPU = avg(TotalCPUCores),\n",
    "    AvgMemory = avg(TotalMemoryGB),\n",
    "    AvgEfficiency = avg(ExecutorEfficiency)\n",
    "    by SessionId, SessionName\n",
    "| order by AvgMemory desc\n",
    "```\n",
    "\n",
    "#### Query 3: Bottleneck Detection\n",
    "```kql\n",
    "FabricSparkResourceUsage_CL\n",
    "| where TimeGenerated > ago(24h)\n",
    "| where BottleneckType != \"none\"\n",
    "| summarize \n",
    "    Count = count(),\n",
    "    AvgSeverity = avg(BottleneckSeverity)\n",
    "    by BottleneckType, WorkspaceName\n",
    "| order by Count desc\n",
    "```\n",
    "\n",
    "#### Query 4: Failed Sessions\n",
    "```kql\n",
    "FabricSparkLivySession_CL\n",
    "| where TimeGenerated > ago(24h)\n",
    "| where State in (\"error\", \"dead\", \"killed\")\n",
    "| project TimeGenerated, WorkspaceName, SessionName, State, Log\n",
    "| order by TimeGenerated desc\n",
    "```\n",
    "\n",
    "### Log Analytics Portal\n",
    "Access your data: [Azure Portal - Log Analytics](https://portal.azure.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fabric-env-1.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
