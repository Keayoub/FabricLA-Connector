{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c617f022",
   "metadata": {},
   "source": [
    "## ðŸ”§ Package Setup (Run This First!)\n",
    "\n",
    "**Important:** After updating the `fabricla_connector` package code, run the cell below to reload modules.\n",
    "\n",
    "This ensures the notebook uses the latest version of the refactored package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c17357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Force module reload for development\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the workspace root (parent of notebooks folder)\n",
    "workspace_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(workspace_root, 'src')\n",
    "\n",
    "# Add to path if not already there\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"âœ… Added to Python path: {src_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Already in Python path: {src_path}\")\n",
    "\n",
    "# Remove all fabricla_connector modules from cache\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('fabricla_connector')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "if modules_to_remove:\n",
    "    print(f\"âœ… Cleared {len(modules_to_remove)} cached module(s)\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸ No cached modules to clear\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY - Now run the cells below\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6ab3d",
   "metadata": {},
   "source": [
    "# FabricLA Connector Framework - Quick Start Example\n",
    "\n",
    "This notebook demonstrates how to use the consolidated FabricLA Connector framework for ingesting Fabric data into Azure Log Analytics.\n",
    "\n",
    "## Features\n",
    "- âœ… Consolidated framework with all notebook patterns\n",
    "- âœ… Enhanced error handling and retry logic\n",
    "- âœ… Size-aware batching for optimal performance\n",
    "- âœ… Comprehensive troubleshooting and reporting\n",
    "- âœ… Support for pipelines, dataflows, and capacity monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Framework Setup ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add framework to path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import framework components - using refactored imports\n",
    "from fabricla_connector.config import get_config\n",
    "from fabricla_connector.api import FabricAPIClient\n",
    "from fabricla_connector.collectors import PipelineDataCollector  # Updated from PipelineCollector\n",
    "from fabricla_connector.collectors import DataflowCollector\n",
    "from fabricla_connector.workflows import collect_and_ingest_pipeline_data_enhanced\n",
    "from fabricla_connector.ingestion import AzureMonitorIngestionClient  # Updated from FabricIngestion\n",
    "from fabricla_connector.utils import create_time_window, within_lookback_minutes\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… FabricLA Connector framework loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "load_dotenv()\n",
    "config = get_config()\n",
    "\n",
    "# Basic configuration\n",
    "workspace_id = config.get(\"FABRIC_WORKSPACE_ID\")\n",
    "lookback_minutes = 1440  # Last 24 hours\n",
    "\n",
    "# Example item IDs (replace with your actual IDs)\n",
    "pipeline_ids = [\n",
    "    # \"your-pipeline-id-here\"\n",
    "]\n",
    "\n",
    "dataflow_ids = [\n",
    "    # \"your-dataflow-id-here\" \n",
    "]\n",
    "\n",
    "# Stream names (must match your DCR configuration)\n",
    "stream_pipeline = \"Custom-FabricPipelineRun_CL\"\n",
    "stream_activity = \"Custom-FabricPipelineActivityRun_CL\"\n",
    "stream_dataflow = \"Custom-FabricDataflowRun_CL\"\n",
    "\n",
    "print(f\"ðŸ“Š Configuration:\")\n",
    "print(f\"  Workspace: {workspace_id}\")\n",
    "print(f\"  Lookback: {lookback_minutes} minutes ({lookback_minutes/1440:.1f} days)\")\n",
    "print(f\"  Pipeline items: {len(pipeline_ids)}\")\n",
    "print(f\"  Dataflow items: {len(dataflow_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Simple Framework Usage (Recommended) ===\n",
    "print(\"ðŸš€ Starting data collection using framework workflow...\")\n",
    "\n",
    "try:\n",
    "    # Use the high-level workflow for pipeline data\n",
    "    if pipeline_ids:\n",
    "        result = collect_and_ingest_pipeline_data_enhanced(\n",
    "            workspace_id=workspace_id,\n",
    "            pipeline_item_ids=pipeline_ids,\n",
    "            lookback_minutes=lookback_minutes,\n",
    "            collect_activity_runs=True,\n",
    "            stream_pipeline=stream_pipeline,\n",
    "            stream_activity=stream_activity,\n",
    "            troubleshoot=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Pipeline data collected:\")\n",
    "        print(f\"   Pipeline runs: {result.get('pipeline_runs_ingested', 0)}\")\n",
    "        print(f\"   Activity runs: {result.get('activity_runs_ingested', 0)}\")\n",
    "    else:\n",
    "        print(\"â­ï¸  No pipeline IDs configured\")\n",
    "        \n",
    "    print(\"\\nðŸŽ¯ Data should appear in Log Analytics within 5-15 minutes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Advanced Framework Usage (Individual Components) ===\n",
    "print(\"ðŸ”§ Advanced usage with individual framework components...\")\n",
    "\n",
    "# Initialize components\n",
    "fabric_client = FabricAPIClient()\n",
    "ingestion_client = FabricIngestion()\n",
    "\n",
    "# Manual collection with custom logic\n",
    "if dataflow_ids:\n",
    "    dataflow_collector = DataflowCollector(fabric_client)\n",
    "    \n",
    "    all_dataflow_runs = []\n",
    "    for dataflow_id in dataflow_ids:\n",
    "        print(f\"Collecting from dataflow: {dataflow_id}\")\n",
    "        \n",
    "        runs = dataflow_collector.collect_dataflow_runs(\n",
    "            workspace_id=workspace_id,\n",
    "            dataflow_id=dataflow_id,\n",
    "            lookback_minutes=lookback_minutes\n",
    "        )\n",
    "        \n",
    "        # Custom filtering (example)\n",
    "        recent_runs = [\n",
    "            run for run in runs\n",
    "            if within_lookback_minutes(\n",
    "                run.get('startTime', ''),\n",
    "                run.get('endTime', ''),\n",
    "                lookback_minutes\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        all_dataflow_runs.extend(recent_runs)\n",
    "        print(f\"  Found {len(runs)} total runs, {len(recent_runs)} recent\")\n",
    "    \n",
    "    # Enhanced ingestion with troubleshooting\n",
    "    if all_dataflow_runs:\n",
    "        result = ingestion_client.ingest_enhanced(\n",
    "            records=all_dataflow_runs,\n",
    "            troubleshoot=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Dataflow ingestion completed:\")\n",
    "        print(f\"   Successful: {result.get('successful_records', 0)}\")\n",
    "        print(f\"   Failed: {result.get('failed_records', 0)}\")\n",
    "        print(f\"   Success rate: {result.get('success_rate', 0):.1f}%\")\n",
    "    else:\n",
    "        print(\"No dataflow runs to ingest\")\n",
    "else:\n",
    "    print(\"No dataflow IDs configured for advanced example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verification and Troubleshooting ===\n",
    "print(\"ðŸ” Framework capabilities for troubleshooting:\")\n",
    "print(\"\\n1. Enhanced Error Handling:\")\n",
    "print(\"   - Detailed HTTP status code handling (401, 403, 404, 429)\")\n",
    "print(\"   - Automatic retry with exponential backoff\")\n",
    "print(\"   - Comprehensive error reporting\")\n",
    "\n",
    "print(\"\\n2. Size-Aware Batching:\")\n",
    "print(\"   - JSON size-based chunking (950KB limit)\")\n",
    "print(\"   - Automatic batch splitting for large payloads\")\n",
    "print(\"   - Performance metrics tracking\")\n",
    "\n",
    "print(\"\\n3. Troubleshooting Reports:\")\n",
    "print(\"   - Detailed ingestion summaries\")\n",
    "print(\"   - Configuration validation\")\n",
    "print(\"   - Actionable error recommendations\")\n",
    "\n",
    "print(\"\\n4. KQL Queries for Verification:\")\n",
    "print(\"```kql\")\n",
    "print(\"// Check table row counts\")\n",
    "print(\"FabricPipelineRun_CL | count\")\n",
    "print(\"FabricDataflowRun_CL | count\")\n",
    "print(\"\")\n",
    "print(\"// Recent data (last 24 hours)\")\n",
    "print(\"FabricPipelineRun_CL | where TimeGenerated > ago(24h) | take 10\")\n",
    "print(\"\\\"\\\"\\\")\")\n",
    "\n",
    "print(\"\\nâœ… Framework provides comprehensive monitoring and troubleshooting capabilities!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
