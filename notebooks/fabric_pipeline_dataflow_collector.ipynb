{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e614cd47",
   "metadata": {},
   "source": [
    "# Fabric Pipeline & Dataflow Monitoring\n",
    "\n",
    "This notebook collects **Pipeline Runs** and **Dataflow Runs** from the Fabric REST APIs and sends them to Azure Log Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Updated Framework Integration ===\n",
    "# Using the consolidated fabricla_connector framework\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the framework to path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import the framework components\n",
    "from fabricla_connector.config import get_config\n",
    "from fabricla_connector.api import FabricAPIClient\n",
    "from fabricla_connector.collectors import PipelineCollector, DataflowCollector\n",
    "from fabricla_connector.workflows import collect_and_ingest_pipeline_data_enhanced\n",
    "from fabricla_connector.ingestion import FabricIngestion\n",
    "from fabricla_connector.utils import within_lookback_minutes, create_time_window\n",
    "\n",
    "print(\"‚úÖ FabricLA Connector framework loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1de72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Framework-Based Configuration ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration through the framework\n",
    "config = get_config()\n",
    "\n",
    "# Workspace and item configuration\n",
    "workspace_id = config.get(\"FABRIC_WORKSPACE_ID\")\n",
    "\n",
    "# List only the items you want to collect. Leave empty lists to skip.\n",
    "pipeline_item_ids = [\n",
    "    # Add your pipeline IDs here, e.g.:\n",
    "    \"fffa88df-ed23-45ba-bb2e-803f0089dc39\",\n",
    "    \"92243a88-c144-4749-8eac-e2dd8e7f9b31\",\n",
    "]\n",
    "dataflow_item_ids = [\n",
    "    # Add your dataflow IDs here, e.g.:\n",
    "    \"bc5c92b0-d58a-487b-8692-965e69345792\",\n",
    "    \"065696af-4621-4538-953c-65899053ae24\",\n",
    "]\n",
    "\n",
    "# === CONFIGURATION MODES ===\n",
    "# OPTION 1: Bulk Ingestion (Historical Data Collection)\n",
    "lookback_minutes = 43200  # 30 days for comprehensive bulk load\n",
    "collect_activity_runs = True  # Enabled for detailed monitoring\n",
    "\n",
    "# OPTION 2: Incremental Collection (Regular Monitoring)  \n",
    "# lookback_minutes = 1200  # 20 hours for regular incremental collection\n",
    "# collect_activity_runs = True  # Enabled for detailed activity monitoring\n",
    "\n",
    "# üéØ OPTION 3: Activity Runs Backfill (After bulk completion)\n",
    "# lookback_minutes = 10080  # 7 days for recent activity runs\n",
    "# collect_activity_runs = True  # Enabled for detailed data\n",
    "\n",
    "# DCR / Logs Ingestion API settings from environment variables (using correct parameter names)\n",
    "dcr_endpoint_host = config.get(\"DCR_ENDPOINT_HOST\")\n",
    "dcr_immutable_id = config.get(\"DCR_IMMUTABLE_ID\")\n",
    "\n",
    "# Stream names must match your DCR configuration and map to the LA tables we created\n",
    "stream_pipeline = \"Custom-FabricPipelineRun_CL\"             \n",
    "stream_activity = \"Custom-FabricPipelineActivityRun_CL\"     \n",
    "stream_dataflow = \"Custom-FabricDataflowRun_CL\"\n",
    "\n",
    "# === Authentication Configuration ===\n",
    "# Basic credentials from environment variables\n",
    "tenant_id = config.get(\"FABRIC_TENANT_ID\")\n",
    "client_id = config.get(\"FABRIC_APP_ID\")\n",
    "client_secret_env = config.get(\"FABRIC_APP_SECRET\")\n",
    "\n",
    "# Key Vault configuration (optional)\n",
    "use_key_vault = False  # Set to True to use Key Vault\n",
    "use_managed_identity = False  # Set to True when running on Azure resources (VM, Container App, etc.)\n",
    "key_vault_uri = config.get(\"AZURE_KEY_VAULT_URI\", \"https://kaydemokeyvault.vault.azure.net/\")\n",
    "key_vault_secret_name = config.get(\"AZURE_KEY_VAULT_SECRET_NAME\", \"FabricServicePrincipal\")\n",
    "\n",
    "# Validation using correct parameter names from .env file\n",
    "required_vars = [\"FABRIC_WORKSPACE_ID\", \"FABRIC_TENANT_ID\", \"FABRIC_APP_ID\", \"DCR_ENDPOINT_HOST\", \"DCR_IMMUTABLE_ID\"]\n",
    "missing = [var for var in required_vars if not config.get(var)]\n",
    "\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing)}\")\n",
    "    print(\"   Please check your .env file or environment configuration\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables loaded successfully\")\n",
    "\n",
    "if not client_secret_env and not use_key_vault:\n",
    "    print(\"‚ö†Ô∏è  Warning: No authentication method configured\")\n",
    "    print(\"   Either set FABRIC_APP_SECRET or configure Key Vault\")\n",
    "\n",
    "print(f\"\\nüìä Data Collection Configuration:\")\n",
    "print(f\"  Workspace ID: {workspace_id or 'Not set'}\")\n",
    "print(f\"  Lookback Window: {lookback_minutes:,} minutes ({lookback_minutes/1440:.1f} days)\")\n",
    "print(f\"  Collect Activity Runs: {collect_activity_runs}\")\n",
    "print(f\"  Pipeline Items: {len(pipeline_item_ids)}\")\n",
    "print(f\"  Dataflow Items: {len(dataflow_item_ids)}\")\n",
    "\n",
    "print(f\"\\nüîó Azure Monitor Configuration:\")\n",
    "print(f\"  DCR Endpoint: {dcr_endpoint_host or 'Not set'}\")\n",
    "print(f\"  DCR Immutable ID: {dcr_immutable_id or 'Not set'}\")\n",
    "print(f\"  Stream Names:\")\n",
    "print(f\"    - Pipeline: {stream_pipeline}\")\n",
    "print(f\"    - Activity: {stream_activity}\")\n",
    "print(f\"    - Dataflow: {stream_dataflow}\")\n",
    "\n",
    "# Store configuration for use in other cells\n",
    "ingestion_config = {\n",
    "    \"dcr_endpoint_host\": dcr_endpoint_host,\n",
    "    \"dcr_immutable_id\": dcr_immutable_id,\n",
    "    \"stream_pipeline\": stream_pipeline,\n",
    "    \"stream_activity\": stream_activity,\n",
    "    \"stream_dataflow\": stream_dataflow\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fe8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Framework-Based Authentication ===\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"üîê Setting up authentication...\")\n",
    "\n",
    "# Initialize the framework components with the correct configuration\n",
    "try:\n",
    "    # Create API client using framework\n",
    "    fabric_client = FabricAPIClient()\n",
    "    \n",
    "    # Initialize ingestion client with the correct DCR parameters\n",
    "    ingestion_client = FabricIngestion(\n",
    "        endpoint_host=f\"https://{dcr_endpoint_host}\",  # Ensure proper URL format\n",
    "        dcr_id=dcr_immutable_id,\n",
    "        stream_name=stream_pipeline  # Default stream, can be overridden per call\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Framework components initialized successfully\")\n",
    "    print(f\"   DCR Endpoint: https://{dcr_endpoint_host}\")\n",
    "    print(f\"   DCR ID: {dcr_immutable_id[:20]}...\")\n",
    "    \n",
    "    # Test authentication\n",
    "    credential = DefaultAzureCredential()\n",
    "    fabric_token = credential.get_token(\"https://api.fabric.microsoft.com/.default\")\n",
    "    monitor_token = credential.get_token(\"https://monitor.azure.com/.default\")\n",
    "    \n",
    "    print(\"‚úÖ Authentication tokens acquired successfully\")\n",
    "    print(f\"   Fabric token: {fabric_token.token[:10]}...{fabric_token.token[-10:]}\")\n",
    "    print(f\"   Monitor token: {monitor_token.token[:10]}...{monitor_token.token[-10:]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication or framework initialization failed: {e}\")\n",
    "    print(\"   Please check your credentials and configuration\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55106c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Variable Validation ===\n",
    "# This cell validates that all required environment variables are properly set\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if .env file exists in current directory or parent directories\n",
    "env_files_found = []\n",
    "for path in ['.env', '../.env', '../../.env']:\n",
    "    if os.path.exists(path):\n",
    "        env_files_found.append(path)\n",
    "\n",
    "print(\"üîç Environment File Detection:\")\n",
    "if env_files_found:\n",
    "    print(f\"   Found .env files: {', '.join(env_files_found)}\")\n",
    "else:\n",
    "    print(\"   No .env files found in current or parent directories\")\n",
    "    print(\"   Make sure to copy .env.example to .env and fill in your values\")\n",
    "\n",
    "# Detailed environment variable status\n",
    "required_vars = [\n",
    "    (\"FABRIC_TENANT_ID\", \"Azure tenant ID\"),\n",
    "    (\"FABRIC_APP_ID\", \"Service principal client ID\"),\n",
    "    (\"FABRIC_APP_SECRET\", \"Service principal client secret\"),\n",
    "    (\"FABRIC_WORKSPACE_ID\", \"Fabric workspace ID\"),\n",
    "    (\"DCR_ENDPOINT_HOST\", \"Data Collection Rule endpoint host\"),\n",
    "    (\"DCR_IMMUTABLE_ID\", \"Data Collection Rule immutable ID\"),\n",
    "]\n",
    "\n",
    "optional_vars = [\n",
    "    (\"LOG_ANALYTICS_WORKSPACE_ID\", \"Log Analytics workspace ID\"),\n",
    "    (\"AZURE_SUBSCRIPTION_ID\", \"Azure subscription ID\"),\n",
    "    (\"FABRIC_RUNTIME_VERSION\", \"Fabric runtime version\"),\n",
    "    (\"AZURE_KEY_VAULT_URI\", \"Azure Key Vault URI\"),\n",
    "    (\"AZURE_KEY_VAULT_SECRET_NAME\", \"Key Vault secret name\"),\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Required Environment Variables:\")\n",
    "for var_name, description in required_vars:\n",
    "    value = os.getenv(var_name)\n",
    "    if value:\n",
    "        # Show first 8 chars for security\n",
    "        display_value = value[:8] + \"...\" if len(value) > 8 else value\n",
    "        print(f\"   ‚úÖ {var_name}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {var_name}: Not set - {description}\")\n",
    "\n",
    "print(\"\\nüìã Optional Environment Variables:\")\n",
    "for var_name, description in optional_vars:\n",
    "    value = os.getenv(var_name)\n",
    "    if value:\n",
    "        display_value = value[:8] + \"...\" if len(value) > 8 else value\n",
    "        print(f\"   ‚úÖ {var_name}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö™ {var_name}: Not set - {description}\")\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   1. Ensure all required variables are set in your .env file\")\n",
    "print(\"   2. Run the parameters cell above to load configuration\")\n",
    "print(\"   3. Proceed with authentication and data collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fabric Runtime Detection ===\n",
    "# This cell detects if we're running in Fabric and adapts accordingly\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect if we're running in Fabric\n",
    "running_in_fabric = False\n",
    "try:\n",
    "    import notebookutils\n",
    "    running_in_fabric = True\n",
    "    print(\"üè≠ Running in Microsoft Fabric environment\")\n",
    "    print(f\"   Fabric notebook utilities available: {notebookutils is not None}\")\n",
    "    \n",
    "    # Try to get workspace context from Fabric\n",
    "    try:\n",
    "        # In Fabric, you can get current workspace info\n",
    "        fabric_workspace_info = notebookutils.credentials.getSecret(\"FabricWorkspace\", \"WorkspaceId\")\n",
    "        if fabric_workspace_info and not workspace_id:\n",
    "            workspace_id = fabric_workspace_info\n",
    "            print(f\"   Using Fabric workspace context: {workspace_id[:8]}...\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"üíª Running in local development environment\")\n",
    "    print(\"   Loading configuration from .env file\")\n",
    "\n",
    "# Check for semantic-link-sempy (available in Fabric)\n",
    "try:\n",
    "    import sempy.fabric as fabric\n",
    "    print(\"‚úÖ Semantic Link available - can use Fabric workspace functions\")\n",
    "    \n",
    "    # Get current workspace if not set\n",
    "    if not workspace_id:\n",
    "        try:\n",
    "            current_workspace = fabric.get_workspace_id()\n",
    "            if current_workspace:\n",
    "                workspace_id = current_workspace\n",
    "                print(f\"   Auto-detected workspace ID: {workspace_id[:8]}...\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "except ImportError:\n",
    "    if running_in_fabric:\n",
    "        print(\"‚ö†Ô∏è  Semantic Link not available in this Fabric runtime\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  Semantic Link not available (local environment)\")\n",
    "\n",
    "# Fabric-specific authentication options\n",
    "if running_in_fabric:\n",
    "    print(\"\\nüîê Fabric Authentication Options:\")\n",
    "    print(\"   1. Use Fabric workspace identity (recommended)\")\n",
    "    print(\"   2. Use Key Vault with workspace managed identity\")\n",
    "    print(\"   3. Set credentials in parameters cell\")\n",
    "    print(\"   4. Use environment variables (if .env file uploaded)\")\n",
    "    \n",
    "    # In Fabric, you can use workspace identity for authentication\n",
    "    try:\n",
    "        # Check if we can use Fabric's built-in authentication\n",
    "        if hasattr(notebookutils, 'credentials'):\n",
    "            print(\"   ‚úÖ Fabric credential utilities available\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Fabric credential utilities not available\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"\\nüîê Local Development Authentication:\")\n",
    "    print(\"   Using environment variables from .env file\")\n",
    "\n",
    "print(f\"\\nüìç Current Configuration:\")\n",
    "print(f\"   Runtime Environment: {'Fabric' if running_in_fabric else 'Local'}\")\n",
    "print(f\"   Workspace ID: {workspace_id[:8] + '...' if workspace_id else 'Not set'}\")\n",
    "print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"   Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da89d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Alternative: Using Individual Framework Components ===\n",
    "# This cell shows how to use framework components individually for custom scenarios\n",
    "\n",
    "\"\"\"\n",
    "# For advanced users who want granular control:\n",
    "\n",
    "# 1. Individual collectors\n",
    "pipeline_collector = PipelineCollector(fabric_client)\n",
    "dataflow_collector = DataflowCollector(fabric_client)\n",
    "\n",
    "# 2. Manual collection with custom filtering\n",
    "pipeline_runs = pipeline_collector.collect_pipeline_runs(\n",
    "    workspace_id=workspace_id,\n",
    "    pipeline_id=\"your-pipeline-id\",\n",
    "    lookback_minutes=lookback_minutes\n",
    ")\n",
    "\n",
    "# 3. Custom ingestion with specific configuration\n",
    "custom_ingestion = FabricIngestion(\n",
    "    endpoint_host=config.get(\"AZURE_MONITOR_DCE_ENDPOINT\"),\n",
    "    dcr_id=config.get(\"AZURE_MONITOR_DCR_IMMUTABLE_ID\"),\n",
    "    stream_name=\"Custom-MyCustomTable_CL\"\n",
    ")\n",
    "\n",
    "# 4. Enhanced ingestion with troubleshooting\n",
    "result = custom_ingestion.ingest_enhanced(\n",
    "    records=pipeline_runs,\n",
    "    troubleshoot=True\n",
    ")\n",
    "\n",
    "# The framework provides flexibility for both simple workflows and advanced scenarios\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí° Framework components available for custom scenarios:\")\n",
    "print(\"   - FabricAPIClient: Low-level Fabric REST API access\")\n",
    "print(\"   - PipelineCollector: Focused pipeline data collection\")\n",
    "print(\"   - DataflowCollector: Focused dataflow data collection\") \n",
    "print(\"   - FabricIngestion: Enhanced ingestion with retry logic\")\n",
    "print(\"   - Workflows: High-level orchestration functions\")\n",
    "print(\"   - Utils: Helper functions for date/time, chunking, validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Collection and Ingestion ===\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "print(\"üöÄ Starting pipeline and dataflow data collection...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize summary for tracking results\n",
    "summary = {\n",
    "    \"collection_timestamp\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"lookback_minutes\": lookback_minutes,\n",
    "    \"workspace_id\": workspace_id,\n",
    "    \"configuration\": {\n",
    "        \"dcr_endpoint_host\": dcr_endpoint_host,\n",
    "        \"dcr_immutable_id\": dcr_immutable_id,\n",
    "        \"pipeline_items\": len(pipeline_item_ids),\n",
    "        \"dataflow_items\": len(dataflow_item_ids)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize collectors\n",
    "pipeline_collector = PipelineCollector(fabric_client)\n",
    "dataflow_collector = DataflowCollector(fabric_client)\n",
    "\n",
    "# === COLLECT PIPELINE DATA ===\n",
    "pipeline_rows = []\n",
    "activity_rows = []\n",
    "\n",
    "if pipeline_item_ids:\n",
    "    print(f\"\\nüìã Collecting pipeline data for {len(pipeline_item_ids)} items...\")\n",
    "    \n",
    "    for pipeline_id in pipeline_item_ids:\n",
    "        print(f\"   Collecting from pipeline: {pipeline_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect pipeline runs\n",
    "            runs = pipeline_collector.collect_pipeline_runs(\n",
    "                workspace_id=workspace_id,\n",
    "                pipeline_id=pipeline_id,\n",
    "                lookback_minutes=lookback_minutes\n",
    "            )\n",
    "            pipeline_rows.extend(runs)\n",
    "            print(f\"     Found {len(runs)} pipeline runs\")\n",
    "            \n",
    "            # Collect activity runs if enabled\n",
    "            if collect_activity_runs:\n",
    "                for run in runs:\n",
    "                    if run.get('id'):\n",
    "                        activities = pipeline_collector.collect_activity_runs(\n",
    "                            workspace_id=workspace_id,\n",
    "                            pipeline_run_id=run['id'],\n",
    "                            lookback_minutes=lookback_minutes\n",
    "                        )\n",
    "                        activity_rows.extend(activities)\n",
    "                        print(f\"     Found {len(activities)} activity runs for pipeline run {run['id'][:8]}...\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Error collecting from pipeline {pipeline_id}: {e}\")\n",
    "            \n",
    "    print(f\"‚úÖ Pipeline collection completed: {len(pipeline_rows)} runs, {len(activity_rows)} activities\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No pipeline IDs configured - skipping pipeline collection\")\n",
    "\n",
    "# === COLLECT DATAFLOW DATA ===\n",
    "dataflow_rows = []\n",
    "\n",
    "if dataflow_item_ids:\n",
    "    print(f\"\\nüîÑ Collecting dataflow data for {len(dataflow_item_ids)} items...\")\n",
    "    \n",
    "    for dataflow_id in dataflow_item_ids:\n",
    "        print(f\"   Collecting from dataflow: {dataflow_id}\")\n",
    "        \n",
    "        try:\n",
    "            runs = dataflow_collector.collect_dataflow_runs(\n",
    "                workspace_id=workspace_id,\n",
    "                dataflow_id=dataflow_id,\n",
    "                lookback_minutes=lookback_minutes\n",
    "            )\n",
    "            dataflow_rows.extend(runs)\n",
    "            print(f\"     Found {len(runs)} dataflow runs\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Error collecting from dataflow {dataflow_id}: {e}\")\n",
    "            \n",
    "    print(f\"‚úÖ Dataflow collection completed: {len(dataflow_rows)} runs\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No dataflow IDs configured - skipping dataflow collection\")\n",
    "\n",
    "# === INGEST DATA TO AZURE MONITOR ===\n",
    "print(f\"\\nüì§ Starting data ingestion to Azure Monitor...\")\n",
    "\n",
    "# Ingest pipeline runs\n",
    "if pipeline_rows:\n",
    "    print(f\"   Ingesting {len(pipeline_rows)} pipeline runs...\")\n",
    "    try:\n",
    "        pipeline_ingestion = FabricIngestion(\n",
    "            endpoint_host=f\"https://{dcr_endpoint_host}\",\n",
    "            dcr_id=dcr_immutable_id,\n",
    "            stream_name=stream_pipeline\n",
    "        )\n",
    "        pipeline_result = pipeline_ingestion.ingest_enhanced(\n",
    "            records=pipeline_rows,\n",
    "            troubleshoot=True\n",
    "        )\n",
    "        summary[\"pipeline_runs\"] = {\n",
    "            \"collected\": len(pipeline_rows),\n",
    "            \"ingested\": pipeline_result.get(\"successful_records\", 0),\n",
    "            \"failed\": pipeline_result.get(\"failed_records\", 0),\n",
    "            \"success_rate\": pipeline_result.get(\"success_rate\", 0)\n",
    "        }\n",
    "        print(f\"     ‚úÖ Pipeline runs: {pipeline_result.get('successful_records', 0)}/{len(pipeline_rows)} ingested\")\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Pipeline ingestion failed: {e}\")\n",
    "        summary[\"pipeline_runs\"] = {\"collected\": len(pipeline_rows), \"ingested\": 0, \"error\": str(e)}\n",
    "\n",
    "# Ingest activity runs\n",
    "if activity_rows:\n",
    "    print(f\"   Ingesting {len(activity_rows)} activity runs...\")\n",
    "    try:\n",
    "        activity_ingestion = FabricIngestion(\n",
    "            endpoint_host=f\"https://{dcr_endpoint_host}\",\n",
    "            dcr_id=dcr_immutable_id,\n",
    "            stream_name=stream_activity\n",
    "        )\n",
    "        activity_result = activity_ingestion.ingest_enhanced(\n",
    "            records=activity_rows,\n",
    "            troubleshoot=True\n",
    "        )\n",
    "        summary[\"activity_runs\"] = {\n",
    "            \"collected\": len(activity_rows),\n",
    "            \"ingested\": activity_result.get(\"successful_records\", 0),\n",
    "            \"failed\": activity_result.get(\"failed_records\", 0),\n",
    "            \"success_rate\": activity_result.get(\"success_rate\", 0)\n",
    "        }\n",
    "        print(f\"     ‚úÖ Activity runs: {activity_result.get('successful_records', 0)}/{len(activity_rows)} ingested\")\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Activity ingestion failed: {e}\")\n",
    "        summary[\"activity_runs\"] = {\"collected\": len(activity_rows), \"ingested\": 0, \"error\": str(e)}\n",
    "\n",
    "# Ingest dataflow runs\n",
    "if dataflow_rows:\n",
    "    print(f\"   Ingesting {len(dataflow_rows)} dataflow runs...\")\n",
    "    try:\n",
    "        dataflow_ingestion = FabricIngestion(\n",
    "            endpoint_host=f\"https://{dcr_endpoint_host}\",\n",
    "            dcr_id=dcr_immutable_id,\n",
    "            stream_name=stream_dataflow\n",
    "        )\n",
    "        dataflow_result = dataflow_ingestion.ingest_enhanced(\n",
    "            records=dataflow_rows,\n",
    "            troubleshoot=True\n",
    "        )\n",
    "        summary[\"dataflow_runs\"] = {\n",
    "            \"collected\": len(dataflow_rows),\n",
    "            \"ingested\": dataflow_result.get(\"successful_records\", 0),\n",
    "            \"failed\": dataflow_result.get(\"failed_records\", 0),\n",
    "            \"success_rate\": dataflow_result.get(\"success_rate\", 0)\n",
    "        }\n",
    "        print(f\"     ‚úÖ Dataflow runs: {dataflow_result.get('successful_records', 0)}/{len(dataflow_rows)} ingested\")\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Dataflow ingestion failed: {e}\")\n",
    "        summary[\"dataflow_runs\"] = {\"collected\": len(dataflow_rows), \"ingested\": 0, \"error\": str(e)}\n",
    "\n",
    "# === SUMMARY REPORT ===\n",
    "total_collected = len(pipeline_rows) + len(activity_rows) + len(dataflow_rows)\n",
    "total_ingested = (\n",
    "    summary.get(\"pipeline_runs\", {}).get(\"ingested\", 0) +\n",
    "    summary.get(\"activity_runs\", {}).get(\"ingested\", 0) +\n",
    "    summary.get(\"dataflow_runs\", {}).get(\"ingested\", 0)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA COLLECTION AND INGESTION COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Total records collected: {total_collected}\")\n",
    "print(f\"üì§ Total records ingested: {total_ingested}\")\n",
    "print(f\"üìã Pipeline runs: {summary.get('pipeline_runs', {}).get('ingested', 0)}\")\n",
    "print(f\"üîÑ Activity runs: {summary.get('activity_runs', {}).get('ingested', 0)}\")\n",
    "print(f\"üíß Dataflow runs: {summary.get('dataflow_runs', {}).get('ingested', 0)}\")\n",
    "\n",
    "if total_ingested > 0:\n",
    "    print(f\"\\nüéØ Data should appear in Log Analytics within 5-15 minutes\")\n",
    "    print(f\"   Tables: {stream_pipeline}, {stream_activity}, {stream_dataflow}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No records ingested - check your configuration and logs above\")\n",
    "\n",
    "print(f\"\\nüìÑ Detailed summary available in 'summary' variable\")\n",
    "print(json.dumps(summary, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Enhanced Troubleshooting with Framework ===\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "print(\"üîç ENHANCED TROUBLESHOOTING REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Display comprehensive summary\n",
    "if 'summary' in locals():\n",
    "    print(\"\\n1. COLLECTION & INGESTION SUMMARY:\")\n",
    "    print(json.dumps(summary, indent=2, default=str))\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_collected = (\n",
    "        summary.get(\"pipeline_runs\", {}).get(\"collected\", 0) +\n",
    "        summary.get(\"activity_runs\", {}).get(\"collected\", 0) +\n",
    "        summary.get(\"dataflow_runs\", {}).get(\"collected\", 0)\n",
    "    )\n",
    "    \n",
    "    total_ingested = (\n",
    "        summary.get(\"pipeline_runs\", {}).get(\"ingested\", 0) +\n",
    "        summary.get(\"activity_runs\", {}).get(\"ingested\", 0) +\n",
    "        summary.get(\"dataflow_runs\", {}).get(\"ingested\", 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"   Total collected: {total_collected}\")\n",
    "    print(f\"   Total ingested: {total_ingested}\")\n",
    "    print(f\"   Success rate: {(total_ingested/total_collected*100) if total_collected > 0 else 0:.1f}%\")\n",
    "    \n",
    "    if total_ingested == 0:\n",
    "        print(\"\\n‚ùå No data was ingested - this explains why tables are empty!\")\n",
    "        print(\"üîç Possible causes:\")\n",
    "        print(\"   - No recent runs in the lookback window\")\n",
    "        print(\"   - Empty item ID lists\")\n",
    "        print(\"   - API authentication issues\")\n",
    "        print(\"   - DCR configuration problems\")\n",
    "        print(\"   - Network connectivity issues\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {total_ingested} records were successfully ingested to DCR\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No summary data available - collection may have failed\")\n",
    "    print(\"   Make sure to run the data collection cell first\")\n",
    "\n",
    "# 2. Configuration verification\n",
    "print(\"\\n2. CONFIGURATION VERIFICATION:\")\n",
    "if 'dcr_endpoint_host' in locals() and 'dcr_immutable_id' in locals():\n",
    "    print(f\"   DCR Endpoint: https://{dcr_endpoint_host}\")\n",
    "    print(f\"   DCR Immutable ID: {dcr_immutable_id}\")\n",
    "    print(f\"   Workspace ID: {workspace_id}\")\n",
    "    print(f\"   Lookback Window: {lookback_minutes} minutes ({lookback_minutes/1440:.1f} days)\")\n",
    "else:\n",
    "    print(\"   ‚ùå Configuration variables not found - run configuration cell first\")\n",
    "\n",
    "# 3. Stream and table mapping\n",
    "print(\"\\n3. LOG ANALYTICS TABLE MAPPING:\")\n",
    "print(\"   Expected tables in your Log Analytics workspace:\")\n",
    "if 'stream_pipeline' in locals():\n",
    "    print(f\"   - {stream_pipeline.replace('Custom-', '').replace('_CL', '_CL')}\")\n",
    "if 'stream_activity' in locals():\n",
    "    print(f\"   - {stream_activity.replace('Custom-', '').replace('_CL', '_CL')}\")\n",
    "if 'stream_dataflow' in locals():\n",
    "    print(f\"   - {stream_dataflow.replace('Custom-', '').replace('_CL', '_CL')}\")\n",
    "\n",
    "print(\"\\n   Stream to Table Mapping:\")\n",
    "if 'stream_pipeline' in locals():\n",
    "    print(f\"   {stream_pipeline} ‚Üí FabricPipelineRun_CL\")\n",
    "if 'stream_activity' in locals():\n",
    "    print(f\"   {stream_activity} ‚Üí FabricPipelineActivityRun_CL\")\n",
    "if 'stream_dataflow' in locals():\n",
    "    print(f\"   {stream_dataflow} ‚Üí FabricDataflowRun_CL\")\n",
    "\n",
    "# 4. KQL queries for verification\n",
    "print(\"\\n4. KQL VERIFICATION QUERIES:\")\n",
    "print(\"   Run these queries in your Log Analytics workspace:\")\n",
    "print(\"   ```kql\")\n",
    "print(\"   // Check table row counts\")\n",
    "print(\"   FabricPipelineRun_CL | count\")\n",
    "print(\"   FabricPipelineActivityRun_CL | count\") \n",
    "print(\"   FabricDataflowRun_CL | count\")\n",
    "print()\n",
    "print(\"   // Check recent data (last 24 hours)\")\n",
    "print(\"   FabricPipelineRun_CL | where TimeGenerated > ago(24h) | take 10\")\n",
    "print(\"   FabricPipelineActivityRun_CL | where TimeGenerated > ago(24h) | take 10\")\n",
    "print(\"   FabricDataflowRun_CL | where TimeGenerated > ago(24h) | take 10\")\n",
    "print()\n",
    "print(\"   // Check data freshness\")\n",
    "print(\"   FabricPipelineRun_CL | summarize max(TimeGenerated)\")\n",
    "print(\"   FabricDataflowRun_CL | summarize max(TimeGenerated)\")\n",
    "print(\"   ```\")\n",
    "\n",
    "# 5. Framework-specific troubleshooting\n",
    "print(\"\\n5. FRAMEWORK TROUBLESHOOTING:\")\n",
    "print(\"   The framework provides enhanced error handling:\")\n",
    "print(\"   - Automatic retry with exponential backoff\")\n",
    "print(\"   - Size-aware batching (950KB JSON limit)\")\n",
    "print(\"   - Detailed HTTP status code handling\")\n",
    "print(\"   - Comprehensive error reporting\")\n",
    "\n",
    "# 6. Sample data inspection\n",
    "if 'pipeline_rows' in locals() and pipeline_rows:\n",
    "    print(\"\\n6. SAMPLE PIPELINE DATA:\")\n",
    "    sample = pipeline_rows[0]\n",
    "    print(\"   First pipeline run structure:\")\n",
    "    for key, value in list(sample.items())[:5]:  # Show first 5 fields\n",
    "        print(f\"     {key}: {value}\")\n",
    "    print(f\"   ... and {len(sample)-5} more fields\")\n",
    "\n",
    "if 'dataflow_rows' in locals() and dataflow_rows:\n",
    "    print(\"\\n   SAMPLE DATAFLOW DATA:\")\n",
    "    sample = dataflow_rows[0]\n",
    "    print(\"   First dataflow run structure:\")\n",
    "    for key, value in list(sample.items())[:5]:  # Show first 5 fields\n",
    "        print(f\"     {key}: {value}\")\n",
    "    print(f\"   ... and {len(sample)-5} more fields\")\n",
    "\n",
    "# 7. Next steps based on results\n",
    "print(\"\\n7. RECOMMENDED NEXT STEPS:\")\n",
    "if 'summary' in locals():\n",
    "    if total_ingested > 0:\n",
    "        print(\"   ‚úÖ Data was ingested successfully\")\n",
    "        print(\"   ‚è∞ Wait 10-15 minutes for Log Analytics processing\")\n",
    "        print(\"   üîç Use the KQL queries above to verify data arrival\")\n",
    "        print(\"   üìä Check TimeGenerated column for data freshness\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No data was ingested - investigate issues:\")\n",
    "        print(\"   1. Verify workspace ID and item IDs are correct\")\n",
    "        print(\"   2. Check authentication and permissions\")\n",
    "        print(\"   3. Verify DCR endpoint and immutable ID\")\n",
    "        print(\"   4. Test with a smaller lookback window\")\n",
    "        print(\"   5. Check if items have runs in the specified time window\")\n",
    "\n",
    "# 8. Environment validation\n",
    "print(\"\\n8. ENVIRONMENT VALIDATION:\")\n",
    "print(\"   Required environment variables status:\")\n",
    "required_env_vars = [\"DCR_ENDPOINT_HOST\", \"DCR_IMMUTABLE_ID\", \"FABRIC_WORKSPACE_ID\", \"FABRIC_TENANT_ID\", \"FABRIC_APP_ID\"]\n",
    "for var in required_env_vars:\n",
    "    value = config.get(var) if 'config' in locals() else None\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"   {status} {var}: {'Set' if value else 'Missing'}\")\n",
    "\n",
    "print(f\"\\n‚è∞ Troubleshooting completed at: {dt.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
    "print(\"üí° If issues persist, check the detailed error messages in the collection cell output above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
